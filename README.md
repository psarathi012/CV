# Technical Content Writer & SEO Expert - CV
## ğŸ‘¨â€ğŸ’» About Me

Iâ€™m a technically fluent **AI Engineer turned Technical Writer & SEO Strategist** with a strong grasp of LLM workflows, product-led growth (PLG), and technical content marketing. At **Maxim AI**, I worked closely with the founding team, leading efforts across **AI R&D, SDK development, and technical content**. I fine-tuned models, built POC applications, designed evaluators using LLM-as-a-judge frameworks, and authored cookbooks and teardown blogs to demystify concepts like RAG, agents, and evaluation pipelines.

With hands-on experience in **technical SEO**, I understand how to turn dense engineering topics into content that ranksâ€”merging clarity, context, and conversion. My work bridges engineering precision with storytelling, making me a great fit for roles that need **someone who can code, write, and optimize** all at once.

## âœï¸ Technical Writing

### ğŸ“˜ Official Documentation

- Authored and maintained the entire [Maxim AI documentation](https://www.getmaxim.ai/docs/introduction/overview), covering SDK usage, agent integrations, evaluator configuration, and platform features.
- Wrote onboarding guides, API references, and observability walkthroughs to support developers building with Maxim AI.

### ğŸ“ Technical Blogs

At **Maxim AI**, an AI evaluation and observability platform, I authored deeply technical blogs aimed at advancing understanding of LLM performance, evaluation methodologies, and system reliability. These articles were part of our product-led growth (PLG) motion and served dual purposes: educating technical audiences and driving adoption of Maxim's SDK and platform tools.

| Blog Title | Description |
|------------|-------------|
| **[LLM Hallucination Detection](https://www.getmaxim.ai/blog/llm-hallucination-detection/)** | Techniques for identifying hallucinated content in LLM outputs using CoT reasoning, reference-based eval, and open-source evaluators. |
| **[Advanced RAG Techniques](https://www.getmaxim.ai/blog/advanced-rag-techniques/)**  | Covers metadata-aware retrieval, custom rerankers, section-aware chunking, and other real-world RAG optimizations. |
| **[Understanding Jailbreaking & Prompt Injection](https://www.getmaxim.ai/blog/jailbreaking-prompt-injection/)** | Teardown of jailbreaking techniques and prompt injection vulnerabilities in LLMs, with mitigation strategies. |
| **[Agent Workflow Memory](https://www.getmaxim.ai/blog/agent-workflow-memory/)**  | Comparison of scratchpad, episodic, and long-term memory in autonomous agent workflows. Includes architecture and use cases. |
| **[LLM as a Jury](https://www.getmaxim.ai/blog/llm-as-a-jury/)**  | Proposed a multi-LLM voting system for generation evaluationâ€”improving reliability over single-judge LLM assessment. |
| **[Synthetic Data Generation](https://www.getmaxim.ai/blog/synthetic-data-generation/)** | Frameworks for generating synthetic evaluation datasets grounded in real-world knowledge bases and domain examples. |

> ğŸ§  All blog content was **deeply technical**, included **code snippets**, **architectural diagrams**, and **research paper teardowns**. These pieces drove product awareness, SEO traffic, and served as educational content for Maxim AI's observability tooling.

## ğŸ“š Cookbooks & Agentic Workflows

At **Maxim AI**, I developed hands-on cookbooks to showcase various RAG techniques, agentic workflows, and observability practices using the Maxim SDK. These cookbooks are designed to help LLM engineers implement evaluation, monitoring, and debugging in real-world LLM applications.

| Title | Description |
|-------|-------------|
| **[LlamaIndex Online Evaluation](https://github.com/maximhq/maxim-cookbooks/tree/main/python/observability-online-eval/llamaindex)** | Demonstrates observability integration with LlamaIndex in a RAG pipeline. Tracks retrieval quality, hallucination metrics, and response scores. |
| **[LangGraph Evaluation Pipeline](https://github.com/maximhq/maxim-cookbooks/tree/main/python/observability-online-eval/langgraph)** | Integrates Maxim SDK with LangGraph agent workflows to log and evaluate performance at each graph node. |
| **[Customer Support Agent (LangChain)](https://github.com/maximhq/maxim-cookbooks/tree/main/python/observability-online-eval/customer-support-agent)** | LangChain-based agent simulating a support assistant with memory, tools, and multi-turn conversation tracking. |
| **[Cooking Assistant with CrewAI](https://github.com/maximhq/maxim-cookbooks/tree/main/python/observability-online-eval/crew-ai/cooking-agent)** | Role-based agent system using CrewAI to plan meals and execute tasks with inter-agent communication and evaluation. |
| **[Hybrid RAG Pipeline](https://github.com/maximhq/maxim-cookbooks/tree/main/python/observability-online-eval/hybrid-rag)** | Implements hybrid retrieval using BM25 and dense vectors with reranking and response validation instrumentation. |
| **[ReACT Agent with Online Feedback](https://github.com/maximhq/maxim-cookbooks/tree/main/python/observability-online-eval/re-ACT-agent)** | Uses ReACT-style reasoning with integrated online evaluation and log-based feedback via the Maxim SDK. |
| **[Swarm Agent Architecture](https://github.com/maximhq/maxim-cookbooks/tree/main/python/observability-online-eval/swarm-agent)** | Demonstrates agent swarming behavior using CrewAI and AutoGen, complete with interaction-level evaluation logic. |

> ğŸ› ï¸ These examples illustrate how to combine RAG, agentic reasoning, and real-time evaluation using Maxim's observability tools.


